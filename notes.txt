2/1/19
Tried UCB exploration with double q-learning. It doesn't consistently find the global optimum, which is really bad for how simple the environment is. 
I think it's because I only use the UCB bonus for action selection. I don't bootstrap from it in the q-learning update. I'll try that next.
...
Still no good. The best I've gotten so far is with simple eps-greedy action selection

2/4/19
Learning failed to happen because of a major bug, not because of how UCB was implemented. An eps-greedy agent (eps=0.3) gets an average return of slightly over 4 on a 3x3 problem. It completely fails on a larger 7x7 problem. This is not unexpected. The probability of finding a reward through random chance is .3^7 = 0.0002.
...
UCB agents also learn the small task but not the larger one. In this case, the agent loses optimism faster for decisions closer to it. So it has no chance to be optimistic about decisions closer to the global optimum. I'm sure I could find an optimism function that decayed slowly enough for learning to occur, but that's not the point of this exercise.

2/6/19
UCB agents stopped exploring past the starting state, and I'm not sure why. Adding a small (epsilon) chance of taking a random action fixes this, and UCB agents learn faster than only eps-greedy agents. 
...
The neural network used for the Q function in neural.py is just a single layer with no bias or activation function. So, it's linear regression. I can't get a more complex model to solve the problem. I think the more complex neural nets require more training time than I can give them on my laptop. I might have to switch to a more complex version of the problem if I want to do tests with RND.
...
One big problem I was having was exploding gradients. Gradient clipping helped convergence.
Another problem I was having was execution time. By changing gamma to be < 1 and capping each episode to relatively few timesteps, this was also fixed.

2/8/19
I've run into the problem of needing to run part of a graph, interact with the environment, and then run the rest of the graph. Specifically, I need the output of the Q network in order to choose an action. Only after the environment responds to that action with a reward can I calculate gradients. There is a feature called "partial run" in Tensorflow that might solve this problem, but it's still experimental. I also don't want to spend the time on it right now. I will have to be ok with running parts of graphs twice per step.
...
"we normalized the intrinsic reward by dividing it by a running estimate of the standard deviations of the intrinsic returns"
This sentence in the paper confused me. To me, "returns" implies episodic rewards, but they explicitly say they use nonepisodic intrinsic rewards. So I thought maybe they were referring to Q-value estimates, since those estimate returns in the episodic case. But in their ppo_agent.py, line #257, it looks like they divide by the standard deviation of intrinsic *rewards*. So that's what I'm going to do. It also makes more sense.

2/9/19
I fixed two major mistakes today that enabled the RND agent to do at least as well as the agent with no intrinsic rewards. The first and biggest mistake was in calculating the intrinsic reward based on the state before the action and not the state resulting from the action. The other mistake was in treating the intrinsic rewards episodically.

2/10/19
In the fully observed case, I encounter a divide-by-zero error when the stats initializer hasn't visited all the states enough times to give them all non-zero variance. This won't be a problem in the partially observed case. But for now, I'll just have to let the random agent act until all states have nonzero variance.
...
The partially observing RND agent with a 3x3 field of view solves both the 3x3 problem (no surprise) and the 7x7 problem (mild surprise). I'm running a test on a 15x15 grid now. If it also solves this, I'll have to double-check whether I'm leaking information to it or cheating somehow. If I'm not, I'll have to rethink my hypothesis that a curiosity-driven agent couldn't effectively explore a space with a large number of aliased states.

2/11/19
It mostly solved the 15x15 grid. My hypothesis is that it could because the optima were all in the corners. Because the agent is naturally attracted to the edge of the map, it's possible that it found the optima when walking along the walls. I'm trying it again with the rewards only occuring in the halfway points between the center and the four corners.
...
It seems like I was right. Moving the rewards to the new locations made the 15x15 problem unsolvable by the RND agent with a 3x3 window. It should be noted that the Q network for the agent has no recurrent component. An agent with some kind of RNN cell would probably be able to solve this problem, but the model complexity would need to scale with the number of aliased states. It might also depend on whether the initial state is consistent or not. My goal is to propose a method of unaliasing with complexity that does not scale with the problem.
...
There are several different ways to go about unaliasing environment states. The first choice to make is whether the unaliasing should persist across episodes within the same run. It seems like they would be much more useful if they did persist. Since the agents' intrinsic motivation is curiosity, it would make sense for them to prefer states that haven't been visited already. 
I considered having the agents leave a (binary) marker on every state they visit. That could be problematic if agents quickly found themselves locally surrounded by states they've already visited. 
Another way would be for the markers to decay over time, allowing states surrounding the initial state to eventually become "unmarked" again. The decay would change the RND intrinsic loss for those observations, though. In general, decaying markers would greatly increase the time required for random network distillation.
Another alternative would be to have discrete markers (reducing distillation time) that disappear with some small probability at each time step (preventing saturation/re-aliasing).
I also must decide how to incorporate these markers into the environment and the agent's observations. They could either be stored as a second 2d "layer" of information for each observation, or they could be combined into the existing observation structure. Basically, the question is whether or not the neural net's input size changes. If it doesn't change, the markers will be interpreted as reward in this specific environment/formulation. It would have to be a small negative reward to promote exploration.
This seems like an approach independent of curiosity, though. The most natural way to pair this technique with curiosity is probably to add another dimension to the observation and double the neural net's input size.
My conclusion seems to be that there are two viable methods to consider. One is making the markers a continuously decaying negative reward embedded in the existing observation dimension. The other is making the markers discrete, possibly disappearing,  and on another observation dimension, for use with curiosity. The one without curiosity is simpler, so I'll try that first.

2/12/19
The negative reward trail at least didn't inhibit the solving of the 3x3 problem. Though, I did have to set the decay rate relatively high (0.4) due to the low number of states in the environment. I did need to increase the rewards in the environment to ensure that the center state was still a local optimum.
...
It also somewhat solves the 15x15 problem, though it doesn't consistently find the global optimum. Still, it outperforms a memory-less RND agent on the same problem. Performance seems to be sensitive to the decay rate parameter. I'm guessing that a lower decay rate gives the Q network a larger set of possible observations, making learning harder. While a lower decay rate might be useful for longer-term memory, a higher decay rate at least prevents agents from doing obviously suboptimal things like turning around. Perhaps a lower decay rate could be feasible with a Q-network with more representational capacity and longer training times.
...
I've now realized that a larger Q-network being necessary for lower decay rates is analagous to needing more RNN capacity for a longer memory span. It's still possible that the negative reward trail scales better than an RNN in number of aliased states, but this would need to be studied. I still need to try the disappearing discrete markers approach. It might work better.
...
The randomly disappearing binary markers (breadcrumbs) idea isn't working well with RND curiosity. My guess is that a consistent decaying trail is easier to learn about than randomness in the agent's wake. I will try using a decaying trail with RND.
...
A decaying trail works sometimes with RND. Interestingly, I've only seen positive results when the decay rate is very low, resulting in long trails with a higher resolution of values. I may have to revisit my theory on the relationship between decay rate and model complexity. 
For some reason, I was reusing the same initialization of parameters for every run. This almost defeats the purpose of having multiple runs. I now re-initialize every time.

2/13/19
It looks like reward trails perform better than feature trails, which is not surprising. The agent must learn what to do with the feature information. None of that learning is required when the trail is directly realized as a decrease in reward.
It also looks like curiosity didn't significantly improve upon either unaliasing method. In fact, it decreased performance significantly when added to a feature trails agent. However, if you consider success a bernoulli random variable conditioned on whether the global optimum was found, curiosity with reward trails found it the most often (8 times out of 40). If you measure success by the number of times the agent found an optimum other than the local optimum, curiosity only adds one more success to the plain reward trails agent.
I should try using discrete markers that persist only throughout a single episode. 
