2/1/19
Tried UCB exploration with double q-learning. It doesn't consistently find the global optimum, which is really bad for how simple the environment is. 
I think it's because I only use the UCB bonus for action selection. I don't bootstrap from it in the q-learning update. I'll try that next.
...
Still no good. The best I've gotten so far is with simple eps-greedy action selection

2/4/19
Learning failed to happen because of a major bug, not because of how UCB was implemented. An eps-greedy agent (eps=0.3) gets an average return of slightly over 4 on a 3x3 problem. It completely fails on a larger 7x7 problem. This is not unexpected. The probability of finding a reward through random chance is .3^7 = 0.0002.
...
UCB agents also learn the small task but not the larger one. In this case, the agent loses optimism faster for decisions closer to it. So it has no chance to be optimistic about decisions closer to the global optimum. I'm sure I could find an optimism function that decayed slowly enough for learning to occur, but that's not the point of this exercise.

2/6/19
UCB agents stopped exploring past the starting state, and I'm not sure why. Adding a small (epsilon) chance of taking a random action fixes this, and UCB agents learn faster than only eps-greedy agents. 
...
The neural network used for the Q function in neural.py is just a single layer with no bias or activation function. So, it's linear regression. I can't get a more complex model to solve the problem. I think the more complex neural nets require more training time than I can give them on my laptop. I might have to switch to a more complex version of the problem if I want to do tests with RND.
...
One big problem I was having was exploding gradients. Gradient clipping helped convergence.
Another problem I was having was execution time. By changing gamma to be < 1 and capping each episode to relatively few timesteps, this was also fixed.

2/8/19
I've run into the problem of needing to run part of a graph, interact with the environment, and then run the rest of the graph. Specifically, I need the output of the Q network in order to choose an action. Only after the environment responds to that action with a reward can I calculate gradients. There is a feature called "partial run" in Tensorflow that might solve this problem, but it's still experimental. I also don't want to spend the time on it right now. I will have to be ok with running parts of graphs twice per step.
...
"we normalized the intrinsic reward by dividing it by a running estimate of the standard deviations of the intrinsic returns"
This sentence in the paper confused me. To me, "returns" implies episodic rewards, but they explicitly say they use nonepisodic intrinsic rewards. So I thought maybe they were referring to Q-value estimates, since those estimate returns in the episodic case. But in their ppo_agent.py, line #257, it looks like they divide by the standard deviation of intrinsic *rewards*. So that's what I'm going to do. It also makes more sense.

2/9/19
I fixed two major mistakes today that enabled the RND agent to do at least as well as the agent with no intrinsic rewards. The first and biggest mistake was in calculating the intrinsic reward based on the state before the action and not the state resulting from the action. The other mistake was in treating the intrinsic rewards episodically. 
