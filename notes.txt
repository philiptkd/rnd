2/1/19
Tried UCB exploration with double q-learning. It doesn't consistently find the global optimum, which is really bad for how simple the environment is. 
I think it's because I only use the UCB bonus for action selection. I don't bootstrap from it in the q-learning update. I'll try that next.
...
Still no good. The best I've gotten so far is with simple eps-greedy action selection

2/4/19
Learning failed to happen because of a major bug, not because of how UCB was implemented. An eps-greedy agent (eps=0.3) gets an average return of slightly over 4 on a 3x3 problem. It completely fails on a larger 7x7 problem. This is not unexpected. The probability of finding a reward through random chance is .3^7 = 0.0002.
...
UCB agents also learn the small task but not the larger one. In this case, the agent loses optimism faster for decisions closer to it. So it has no chance to be optimistic about decisions closer to the global optimum. I'm sure I could find an optimism function that decayed slowly enough for learning to occur, but that's not the point of this exercise.

2/6/19
UCB agents stopped exploring past the starting state, and I'm not sure why. Adding a small (epsilon) chance of taking a random action fixes this, and UCB agents learn faster than only eps-greedy agents. 
...
The neural network used for the Q function in neural.py is just a single layer with no bias or activation function. So, it's linear regression. I can't get a more complex model to solve the problem. I think the more complex neural nets require more training time than I can give them on my laptop. I might have to switch to a more complex version of the problem if I want to do tests with RND.
...
One big problem I was having was exploding gradients. Gradient clipping helped convergence.
Another problem I was having was execution time. By changing gamma to be < 1 and capping each episode to relatively few timesteps, this was also fixed.

2/8/19
I've run into the problem of needing to run part of a graph, interact with the environment, and then run the rest of the graph. Specifically, I need the output of the Q network in order to choose an action. Only after the environment responds to that action with a reward can I calculate gradients. There is a feature called "partial run" in Tensorflow that might solve this problem, but it's still experimental. I also don't want to spend the time on it right now. I will have to be ok with running parts of graphs twice per step.
...
"we normalized the intrinsic reward by dividing it by a running estimate of the standard deviations of the intrinsic returns"
This sentence in the paper confused me. To me, "returns" implies episodic rewards, but they explicitly say they use nonepisodic intrinsic rewards. So I thought maybe they were referring to Q-value estimates, since those estimate returns in the episodic case. But in their ppo_agent.py, line #257, it looks like they divide by the standard deviation of intrinsic *rewards*. So that's what I'm going to do. It also makes more sense.

2/9/19
I fixed two major mistakes today that enabled the RND agent to do at least as well as the agent with no intrinsic rewards. The first and biggest mistake was in calculating the intrinsic reward based on the state before the action and not the state resulting from the action. The other mistake was in treating the intrinsic rewards episodically.

2/10/19
In the fully observed case, I encounter a divide-by-zero error when the stats initializer hasn't visited all the states enough times to give them all non-zero variance. This won't be a problem in the partially observed case. But for now, I'll just have to let the random act until all states have nonzero variance.
...
The partially observing RND agent with a 3x3 field of view solves both the 3x3 problem (no surprise) and the 7x7 problem (mild surprise). I'm running a test on a 15x15 grid now. If it also solves this, I'll have to double-check whether I'm leaking information to it or cheating somehow. If I'm not, I'll have to rethink my hypothesis that a curiosity-driven agent couldn't effectively explore a space with a large number of aliased states.

2/11/19
It mostly solved the 15x15 grid. My hypothesis is that it could because the optima were all in the corners. Because the agent is naturally attracted to the edge of the map, it's possible that it found the optima when walking along the walls. I'm trying it again with the rewards only occuring in the halfway points between the center and the four corners.
...
It seems like I was right. Moving the rewards to the new locations made the 15x15 problem unsolvable by the RND agent with a 3x3 window. It should be noted that the Q network for the agent has no recurrent component. An agent with some kind of RNN cell would probably be able to solve this problem, but the model complexity would need to scale with the number of aliased states. It might also depend on whether the initial state is consistent or not. My goal is to propose a method of unaliasing with complexity that does not scale with the problem.
