2/1/19
Tried UCB exploration with double q-learning. It doesn't consistently find the global optimum, which is really bad for how simple the environment is. 
I think it's because I only use the UCB bonus for action selection. I don't bootstrap from it in the q-learning update. I'll try that next.
...
Still no good. The best I've gotten so far is with simple eps-greedy action selection

2/4/19
Learning failed to happen because of a major bug, not because of how UCB was implemented. An eps-greedy agent (eps=0.3) gets an average return of slightly over 4 on a 3x3 problem. It completely fails on a larger 7x7 problem. This is not unexpected. The probability of finding a reward through random chance is .3^7 = 0.0002.
...
UCB agents also learn the small task but not the larger one. In this case, the agent loses optimism faster for decisions closer to it. So it has no chance to be optimistic about decisions closer to the global optimum. I'm sure I could find an optimism function that decayed slowly enough for learning to occur, but that's not the point of this exercise.

2/6/19
UCB agents stopped exploring past the starting state, and I'm not sure why. Adding a small (epsilon) chance of taking a random action fixes this, and UCB agents learn faster than only eps-greedy agents. 
...
The neural network used for the Q function in neural.py is just a single layer with no bias or activation function. So, it's linear regression. I can't get a more complex model to solve the problem. I think the more complex neural nets require more training time than I can give them on my laptop. I might have to switch to a more complex version of the problem if I want to do tests with RND.
