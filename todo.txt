try with multiple agents
compare exploration of RND agent with random agent
replace q-learning with multi-step algorithm to propagate exploration bonuses faster
compare with RNN agent on model complexity
try having the trail signals be learned rather than fixed
try having discrete breadcrumbs that only persist within a given episode
