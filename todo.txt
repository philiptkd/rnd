add unaliasing
	random binary string
	try with single agent and multiagent
	try centralized and decentralized with multiagent

collect and analyze data
	percent of state space explored
	average per-agent regret or return
	number of times another optimum was found	

create visualizations
	plot results
	video with decaying trail
	video comparing behavior of different kinds of agents


RND
----------
target network: fixed and randomly initialized
predictor network: same architecture as target. trained to minimize MSE between its and target's outputs, given observations as inputs
the MSE above is the intrinsic reward for the agent
the agent's network needs two heads, one each for extrinsic and intrinsic rewards
reward normalization: divide intrinsic reward by running estimate of its standard deviation
observation normalization: whiten each dimension. subtract running mean and divide by running standard deviation. clip in [-5, 5]
	initialize the means, stddevs for normalization with a few steps of a random agent.
	used only for target and predictor networks, not policy network
----------
compare exploration of RND agent with random agent


